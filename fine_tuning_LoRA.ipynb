{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", 'r') as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import import_data_from_json\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "airbus_datapath = os.path.join(\"./data/\", \"airbus_helicopters_train_set.json\")\n",
    "train_dataset, val_dataset, test_dataset = import_data_from_json(airbus_datapath)\n",
    "\n",
    "# model_name = \"google-t5/t5-small\"\n",
    "# model_name = \"facebook/bart-small\"\n",
    "# model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "#setting padding instructions for tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandreabela/Desktop/Text_Summarization/text_summarization_venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f1c4e4f36547b3b0b4dfdb97e91fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316c79d30700490383e2cf576343a358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandreabela/Desktop/Text_Summarization/text_summarization_venv/lib/python3.11/site-packages/accelerate/accelerator.py:437: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba565dfba4ee4f438cda5f95919eb765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory output_ft/checkpoint-25 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-75 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-125 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-175 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-225 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-250 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 357.5158, 'train_samples_per_second': 2.797, 'train_steps_per_second': 0.699, 'train_loss': 0.5465741577148437, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.5465741577148437, metrics={'train_runtime': 357.5158, 'train_samples_per_second': 2.797, 'train_steps_per_second': 0.699, 'train_loss': 0.5465741577148437, 'epoch': 10.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the trainer\n",
    "from utils import prompt_instruction_format\n",
    "\n",
    "trainingArgs = TrainingArguments(**config['parameters_ft'])\n",
    "\n",
    "peft_config = LoraConfig(**config['parameters_LoRA'])\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=prompt_instruction_format,\n",
    "    args=trainingArgs,\n",
    "    max_seq_length=512\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "    Risks of loss or damage to the Equipment shall pass: From the Purchaser to the Supplier upon delivery as per Article 5, after the signature of the Reception Document; From the Supplier to the Purchaser upon return as per Article 6, after the signature of the Return Document.\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Risk of loss or damage to the Equipment shall pass: From the Purchaser to the Supplier upon delivery, From the Supplier to the Purchaser upon return\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Risks of loss or damage to the Equipment shall pass: From the Purchaser to the Supplier\n"
     ]
    }
   ],
   "source": [
    "def predict_from_model(model, legal_text, device): \n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # prompt = f\"\"\"\n",
    "    # Summarize the following legal text.\n",
    "\n",
    "    # {legal_text}\n",
    "\n",
    "    # Summary:\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {legal_text}\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        output_ids = model.generate(**inputs)\n",
    "        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return output, prompt\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "index = 1\n",
    "\n",
    "legal_text = test_dataset['original_text'][index]\n",
    "summary = test_dataset['reference_summary'][index]\n",
    "\n",
    "output, prompt = predict_from_model(model, legal_text, device)\n",
    "\n",
    "dash_line = '-'.join('' for _ in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandreabela/Desktop/Text_Summarization/text_summarization_venv/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rouge': {'rouge1': {'precision': '0.33373947998609704', 'recall': '0.5656671798478522', 'fmeasure': '0.37287924055321575'}, 'rouge2': {'precision': '0.19779041111828605', 'recall': '0.32981260880420543', 'fmeasure': '0.2171983821513787'}, 'rougeL': {'precision': '0.28986354281565446', 'recall': '0.4957283496149042', 'fmeasure': '0.32417701327042575'}}, 'Similarity': {'similarity_with_reference_summary': {'mean': '0.62909263', 'median': '0.69479036', 'std': '0.25737685'}, 'similarity_with_original_text': {'mean': '0.6864568', 'median': '0.7865524', 'std': '0.24224243'}}}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "device = torch.device('mps')\n",
    "semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def compute_rouge(test_dataset : Dataset, model) -> dict[str, dict[str, str]]:\n",
    "    scores = []\n",
    "    metrics = ['rouge1', 'rouge2', 'rougeL']\n",
    "    scorer = rouge_scorer.RougeScorer(\n",
    "        metrics,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "        \n",
    "    for i in range(len(test_dataset)):\n",
    "        example = test_dataset[i]  \n",
    "\n",
    "        original_text = example['original_text']\n",
    "        reference_summary = example['reference_summary']\n",
    "        generated_summary, _ = predict_from_model(model, original_text, device)\n",
    "\n",
    "        scores.append(\n",
    "              scorer.score(\n",
    "                    generated_summary,\n",
    "                    reference_summary\n",
    "              )\n",
    "        )\n",
    "\n",
    "    final_scores = {\n",
    "        metric: {\n",
    "            \"precision\": str(np.mean(\n",
    "                [score.get(metric).precision for score in scores]\n",
    "            )),\n",
    "            \"recall\": str(np.mean(\n",
    "                [score.get(metric).recall for score in scores]\n",
    "            )),\n",
    "            \"fmeasure\": str(np.mean(\n",
    "                [score.get(metric).fmeasure for score in scores]\n",
    "            )),\n",
    "            }\n",
    "        for metric in metrics\n",
    "    }\n",
    "    return final_scores\n",
    "\n",
    "def compute_similarity_scores(test_dataset : Dataset, model) -> dict[str, dict[str, str]]:\n",
    "    scores_with_reference = []\n",
    "    scores_with_original_text = []\n",
    "\n",
    "    for i in range(len(test_dataset)):\n",
    "        example = test_dataset[i]  \n",
    "\n",
    "        original_text = example['original_text']\n",
    "        reference_summary = example['reference_summary']\n",
    "        generated_summary, _ = predict_from_model(model, original_text, device)\n",
    "\n",
    "        reference_embeddings = semantic_model.encode(\n",
    "            reference_summary,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        original_text_embeddings = semantic_model.encode(\n",
    "            original_text,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        generated_embeddings = semantic_model.encode(\n",
    "            generated_summary,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "\n",
    "        scores_with_reference.append(\n",
    "            util.cos_sim(\n",
    "                reference_embeddings,\n",
    "                generated_embeddings).cpu()\n",
    "        )\n",
    "        scores_with_original_text.append(\n",
    "            util.cos_sim(\n",
    "                original_text_embeddings,\n",
    "                generated_embeddings).cpu()\n",
    "        )\n",
    "            \n",
    "    final_scores = {\n",
    "        \"similarity_with_reference_summary\": {\n",
    "            \"mean\":   str(np.mean(scores_with_reference)),\n",
    "            \"median\": str(np.median(scores_with_reference)),\n",
    "            \"std\":    str(np.std(scores_with_reference))\n",
    "        },\n",
    "        \"similarity_with_original_text\": {\n",
    "            \"mean\":   str(np.mean(scores_with_original_text)),\n",
    "            \"median\": str(np.median(scores_with_original_text)),\n",
    "            \"std\":    str(np.std(scores_with_original_text))\n",
    "        }\n",
    "    }\n",
    "    return final_scores\n",
    "\n",
    "def evaluate(test_dataset : Dataset, model):\n",
    "    model.eval()\n",
    "    return {\n",
    "        \"Rouge\": compute_rouge(test_dataset, model),\n",
    "        \"Similarity\": compute_similarity_scores(test_dataset, model)\n",
    "    }\n",
    "\n",
    "performance_metrics = evaluate(test_dataset, model)\n",
    "print(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42738857278400744\n"
     ]
    }
   ],
   "source": [
    "def final_score(dict_performance : dict):\n",
    "    score = 0\n",
    "\n",
    "    performance_rouge = dict_performance['Rouge']\n",
    "    performance_similarity = dict_performance['Similarity']\n",
    "\n",
    "    for metric in performance_rouge.values():\n",
    "        score += float(metric['precision'])\n",
    "\n",
    "    for metric in performance_similarity.values(): \n",
    "        score += float(metric['mean'])\n",
    "\n",
    "    return score / (len(performance_rouge)+len(performance_similarity))\n",
    "\n",
    "score = final_score(performance_metrics)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Custom class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_similarity_scores_text\n",
    "\n",
    "class TextSummarizer(): \n",
    "    def __init__(self, config_file_path):\n",
    "        with open(config_file_path, 'r') as file:\n",
    "            try:    \n",
    "                self.config = yaml.safe_load(file)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "\n",
    "        # With cuda\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # With MPS (Mac Silicon)\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        \n",
    "        ### Partie LLM Summarization\n",
    "        self.n_brut_force = config['parameters_textsum_archi']['n_brut_force']\n",
    "        models_params = self.config['models_params']\n",
    "        self.models_no_RAG = self.create_models_out_of_RAG(models_params, self.device)\n",
    "\n",
    "        ### Partie LLM + RAG\n",
    "        datapath = self.config['dbRAG']['datapath']\n",
    "        # Load RAG with LangChain\n",
    "\n",
    "    @staticmethod\n",
    "    def create_models_out_of_RAG(models_params : list[tuple], device): \n",
    "        list_models = []\n",
    "\n",
    "        for tokenizer_name, model_path in models_params:\n",
    "\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "            model.eval()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.padding_side = \"right\"\n",
    "\n",
    "            list_models.append((tokenizer, model))\n",
    "        \n",
    "        return list_models\n",
    "    \n",
    "    def predict_no_RAG(self, text_to_summarize):\n",
    "        list_output = []\n",
    "\n",
    "        for tokenizer, model in self.models_no_RAG:\n",
    "            model.eval()\n",
    "            model.to(self.device)\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Summarize the following legal text.\n",
    "\n",
    "            {text_to_summarize}\n",
    "\n",
    "            Summary:\n",
    "            \"\"\"\n",
    "\n",
    "            inputs = tokenizer(prompt, return_tensors='pt')\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            list_output_model = [] \n",
    "            for _ in range(self.n_brut_force): \n",
    "                with torch.no_grad(): \n",
    "                    output_ids = model.generate(**inputs)\n",
    "                    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                sim_score_output = compute_similarity_scores_text(text_to_summarize, output)\n",
    "                \n",
    "                list_output_model.append((sim_score_output, output))\n",
    "\n",
    "            list_output.append(max(list_output_model, key=lambda x: x[0]))\n",
    "\n",
    "        return max(list_output, key=lambda x: x[0])\n",
    "    \n",
    "    def predict_with_RAG(self, text_to_summarize): \n",
    "        sim_score_output, output = self.predict_no_RAG(text_to_summarize)\n",
    "\n",
    "        if sim_score_output > 0.85: \n",
    "            return output\n",
    "        \n",
    "        else: \n",
    "            ### Partie LLM + RAG : Enrichissement du résumé\n",
    "            pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_summarization_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
