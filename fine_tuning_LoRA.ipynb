{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", 'r') as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import import_data_from_json\n",
    "\n",
    "device = torch.device('mps')\n",
    "\n",
    "# airbus_datapath = os.path.join(\"./data/\", \"airbus_helicopters_train_set.json\")\n",
    "# train_dataset, val_dataset, test_dataset = import_data_from_json(airbus_datapath)\n",
    "\n",
    "model_name = \"google-t5/t5-small\"\n",
    "# model_name = \"facebook/bart-base\"\n",
    "# model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "model.save_pretrained(\"./weight/google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandreabela/Desktop/Text_Summarization/text_summarization_venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f1c4e4f36547b3b0b4dfdb97e91fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316c79d30700490383e2cf576343a358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandreabela/Desktop/Text_Summarization/text_summarization_venv/lib/python3.11/site-packages/accelerate/accelerator.py:437: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba565dfba4ee4f438cda5f95919eb765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory output_ft/checkpoint-25 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-50 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-75 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-100 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-125 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-150 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-175 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-200 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-225 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory output_ft/checkpoint-250 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 357.5158, 'train_samples_per_second': 2.797, 'train_steps_per_second': 0.699, 'train_loss': 0.5465741577148437, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.5465741577148437, metrics={'train_runtime': 357.5158, 'train_samples_per_second': 2.797, 'train_steps_per_second': 0.699, 'train_loss': 0.5465741577148437, 'epoch': 10.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the trainer\n",
    "from utils import prompt_instruction_format_t5, prompt_instruction_format_bart\n",
    "\n",
    "trainingArgs = TrainingArguments(**config['parameters_ft'])\n",
    "\n",
    "peft_config = LoraConfig(**config['parameters_LoRA'])\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    # formatting_func=prompt_instruction_format_bart, \n",
    "    formatting_func=prompt_instruction_format_t5,\n",
    "    args=trainingArgs,\n",
    "    max_seq_length=512\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "    Risks of loss or damage to the Equipment shall pass: From the Purchaser to the Supplier upon delivery as per Article 5, after the signature of the Reception Document; From the Supplier to the Purchaser upon return as per Article 6, after the signature of the Return Document.\n",
      "    \n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Risk of loss or damage to the Equipment shall pass: From the Purchaser to the Supplier upon delivery, From the Supplier to the Purchaser upon return\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Risks of loss or damage to the Equipment shall pass: From the Purchaser to the Supplier\n"
     ]
    }
   ],
   "source": [
    "def predict_from_model(model, legal_text, device): \n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # prompt = f\"\"\"\n",
    "    # Summarize the following legal text.\n",
    "\n",
    "    # {legal_text}\n",
    "\n",
    "    # Summary:\n",
    "    # \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {legal_text}\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        output_ids = model.generate(**inputs)\n",
    "        output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return output, prompt\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "index = 1\n",
    "\n",
    "legal_text = test_dataset['original_text'][index]\n",
    "summary = test_dataset['reference_summary'][index]\n",
    "\n",
    "output, prompt = predict_from_model(model, legal_text, device)\n",
    "\n",
    "dash_line = '-'.join('' for _ in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Custom class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "from loguru import logger\n",
    "from utils import compute_similarity_scores_text\n",
    "\n",
    "class TextSummarizer(): \n",
    "    def __init__(self, config_file_path):\n",
    "        with open(config_file_path, 'r') as file:\n",
    "            try:    \n",
    "                self.config = yaml.safe_load(file)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(exc)\n",
    "        logger.info(\"Config info loaded\")\n",
    "\n",
    "        # With cuda\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # With MPS (Mac Silicon)\n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        \n",
    "        ### Partie LLM Summarization\n",
    "        self.n_brut_force = self.config['parameters_textsum_archi']['n_brut_force']\n",
    "        models_params = self.config['models']['models_params']\n",
    "        self.models_no_RAG = self.create_models_out_of_RAG(models_params, self.device)\n",
    "        logger.info(\"Models loaded\")\n",
    "\n",
    "        ### Partie LLM + RAG\n",
    "        datapath = self.config['dbRAG']['datapath']\n",
    "        # Load RAG with LangChain\n",
    "\n",
    "    @staticmethod\n",
    "    def create_models_out_of_RAG(models_params : list[tuple], device): \n",
    "        list_models = []\n",
    "\n",
    "        for tokenizer_name, model_path in models_params:\n",
    "\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "            model.eval()\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.padding_side = \"right\"\n",
    "\n",
    "            list_models.append((tokenizer, model))\n",
    "        \n",
    "        return list_models\n",
    "    \n",
    "    def __call__(self, text_to_summarize):\n",
    "        return self.predict_no_RAG(text_to_summarize)\n",
    "\n",
    "    def predict_no_RAG(self, text_to_summarize):\n",
    "        list_output = []\n",
    "\n",
    "        for tokenizer, model in self.models_no_RAG:\n",
    "            model.eval()\n",
    "            model.to(self.device)\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            {text_to_summarize}\n",
    "            \"\"\"\n",
    "                \n",
    "            inputs = tokenizer(prompt, return_tensors='pt')\n",
    "            inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "\n",
    "            list_output_model = [] \n",
    "            for _ in range(self.n_brut_force): \n",
    "                with torch.no_grad(): \n",
    "                    output_ids = model.generate(**inputs)\n",
    "                    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                sim_score_output = compute_similarity_scores_text(text_to_summarize, output)\n",
    "\n",
    "                if sim_score_output>0.85: \n",
    "                    return (sim_score_output, output)\n",
    "                \n",
    "                list_output_model.append((sim_score_output, output))\n",
    "\n",
    "            list_output.append(max(list_output_model, key=lambda x: x[0]))\n",
    "\n",
    "        return max(list_output, key=lambda x: x[0])\n",
    "    \n",
    "    def predict_with_RAG(self, text_to_summarize): \n",
    "        sim_score_output, output = self.predict_no_RAG(text_to_summarize)\n",
    "\n",
    "        if sim_score_output > 0.85: \n",
    "            return output\n",
    "        \n",
    "        else: \n",
    "            ### Partie LLM + RAG : Enrichissement du résumé\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation custom_class FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import import_data_from_json\n",
    "\n",
    "airbus_datapath = os.path.join(\"./data/\", \"airbus_helicopters_train_set.json\")\n",
    "train_dataset, val_dataset, test_dataset = import_data_from_json(airbus_datapath)\n",
    "\n",
    "text_summarizer = TextSummarizer(config_file_path=\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandreabela/Desktop/Text_Summarization/text_summarization_venv/lib/python3.11/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        rouge1               rouge2               rougeL\n",
      "precision  0.19112413139873094  0.11657675657675659  0.17368588695819817\n",
      "recall      0.5599999999999999   0.3333333333333333                0.495\n",
      "fmeasure      0.27005772005772  0.16266322136089578  0.24355459355459352\n",
      "       similarity_with_reference_summary similarity_with_original_text\n",
      "mean                          0.55679625                    0.65063244\n",
      "median                         0.6943444                     0.6770458\n",
      "std                           0.20050384                   0.066222034\n",
      "Score rouge is 0.2254251783244031\n",
      "Score sim is 0.58807498\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluate\n",
    "\n",
    "score_rouge, score_sim, perf_dict = evaluate(test_dataset, text_summarizer)\n",
    "\n",
    "print(pd.DataFrame(perf_dict['Rouge']))\n",
    "print(pd.DataFrame(perf_dict['Similarity']))\n",
    "print(f\"Score rouge is {score_rouge}\")\n",
    "print(f\"Score sim is {score_sim}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_summarization_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
