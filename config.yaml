parameters_ft: 
  learning_rate: 0.0001
  num_train_epochs: 10
  per_device_train_batch_size: 4
  save_strategy: "steps"
  save_steps: 0.1
  output_dir: "output_ft"

parameters_LoRA:
  lora_alpha: 16
  lora_dropout: 0.1
  r: 16
  bias: "none"
  task_type: "CAUSAL_LM"

models:
  number: 3
  models_names: ["google/flan-t5-base", "facebook/bart-base", "HuggingFaceH4/zephyr-7b-beta"]